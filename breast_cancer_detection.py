# -*- coding: utf-8 -*-
"""Breast-Cancer_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Huo-7g-9GpWX_jSb0bWkb20sfK8UPmII
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from xgboost import XGBClassifier

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

data.head()

data.shape

data.info()

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

data.head()

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

data.tail()

diagnosis_counts = data['diagnosis'].value_counts()
print(diagnosis_counts)

# Step 5: Split data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Step 6: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 7: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 8: Train a Random Forest Classifier (Baseline Model)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Step 9: Make predictions
y_pred = rf_model.predict(X_test)

# Step 10: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("\nModel Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 8: Hyperparameter tuning for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_rf = grid_search.best_estimator_

# Step 9: Make predictions
y_pred = best_rf.predict(X_test)

# Step 10: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("\nBest Model Parameters:", grid_search.best_params_)
print("\nModel Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from xgboost import XGBClassifier

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Split data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Step 6: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 7: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 8: Hyperparameter tuning for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = grid_search_rf.best_estimator_

# Step 9: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = grid_search_xgb.best_estimator_

# Step 10: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)

# Step 11: Evaluate both models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost"], [y_pred_rf, y_pred_xgb]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

print("\nBest Model Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Model Parameters (XGBoost):", grid_search_xgb.best_params_)

for model, scores in metrics.items():
    print(f"\n{model} Performance:")
    for metric, value in scores.items():
        print(f"{metric}: {value:.4f}")
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from xgboost import XGBClassifier

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Split data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Step 6: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 7: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 8: Hyperparameter tuning for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = grid_search_rf.best_estimator_

# Step 9: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = grid_search_xgb.best_estimator_

# Step 10: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 11: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)

# Step 12: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble"], [y_pred_rf, y_pred_xgb, y_pred_ensemble]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

print("\nBest Model Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Model Parameters (XGBoost):", grid_search_xgb.best_params_)

for model, scores in metrics.items():
    print(f"\n{model} Performance:")
    for metric, value in scores.items():
        print(f"{metric}: {value:.4f}")
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from xgboost import XGBClassifier

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Split data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Step 6: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 7: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 8: Hyperparameter tuning for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = grid_search_rf.best_estimator_

# Step 9: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = grid_search_xgb.best_estimator_

# Step 10: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 11: Implement Stacking Classifier
meta_clf = LogisticRegression()
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 12: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 13: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

print("\nBest Model Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Model Parameters (XGBoost):", grid_search_xgb.best_params_)

for model, scores in metrics.items():
    print(f"\n{model} Performance:")
    for metric, value in scores.items():
        print(f"{metric}: {value:.4f}")
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from xgboost import XGBClassifier

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Split data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Step 6: Feature Selection using Recursive Feature Elimination (RFE)
rfe_selector = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=15, step=1)
X_selected = rfe_selector.fit_transform(X, y)
selected_features = X.columns[rfe_selector.support_]
print("Selected Features:", selected_features)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 9: Hyperparameter tuning for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = grid_search_rf.best_estimator_

# Step 10: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = grid_search_xgb.best_estimator_

# Step 11: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 12: Implement Stacking Classifier
meta_clf = LogisticRegression()
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 13: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 14: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

print("\nBest Model Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Model Parameters (XGBoost):", grid_search_xgb.best_params_)

for model, scores in metrics.items():
    print(f"\n{model} Performance:")
    for metric, value in scores.items():
        print(f"{metric}: {value:.4f}")
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Split data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Step 6: Feature Selection using Recursive Feature Elimination (RFE)
rfe_selector = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=15, step=1)
X_selected = rfe_selector.fit_transform(X, y)
selected_features = X.columns[rfe_selector.support_]
print("Selected Features:", selected_features)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 9: Hyperparameter tuning for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = grid_search_rf.best_estimator_

# Step 10: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = grid_search_xgb.best_estimator_

# Step 11: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 12: Implement Stacking Classifier
meta_clf = LogisticRegression()
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 13: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 14: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

print("\nBest Model Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Model Parameters (XGBoost):", grid_search_xgb.best_params_)

for model, scores in metrics.items():
    print(f"\n{model} Performance:")
    for metric, value in scores.items():
        print(f"{metric}: {value:.4f}")
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))

# Step 15: Feature Importance Visualization
feature_importance = best_rf.feature_importances_
sns.barplot(x=feature_importance, y=selected_features)
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance for Random Forest")
plt.show()

# Step 16: ROC Curve
plt.figure(figsize=(10, 6))
for model_name, model in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [best_rf, best_xgb, voting_clf, stacking_clf]):
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.4f}")

plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Split data into features (X) and target (y)
X = data.drop(columns=['diagnosis'])
y = data['diagnosis']

# Step 6: Feature Selection using Recursive Feature Elimination (RFE)
rfe_selector = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=15, step=1)
X_selected = rfe_selector.fit_transform(X, y)
selected_features = X.columns[rfe_selector.support_]
print("Selected Features:", selected_features)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 9: Hyperparameter tuning for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = grid_search_rf.best_estimator_

# Step 10: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = grid_search_xgb.best_estimator_

# Step 11: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 12: Implement Stacking Classifier
meta_clf = LogisticRegression()
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 13: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 14: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

print("\nBest Model Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Model Parameters (XGBoost):", grid_search_xgb.best_params_)

for model, scores in metrics.items():
    print(f"\n{model} Performance:")
    for metric, value in scores.items():
        print(f"{metric}: {value:.4f}")
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))

# Step 15: Feature Importance Visualization
feature_importance = best_rf.feature_importances_
sns.barplot(x=feature_importance, y=selected_features)
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance for Random Forest")
plt.show()

# Step 16: ROC Curve
plt.figure(figsize=(10, 6))
for model_name, model in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [best_rf, best_xgb, voting_clf, stacking_clf]):
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.4f})")

plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neural_network import MLPClassifier
import shap

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Feature Selection using Mutual Information
mi_scores = mutual_info_classif(data.drop(columns=['diagnosis']), data['diagnosis'])
mi_series = pd.Series(mi_scores, index=data.drop(columns=['diagnosis']).columns)
top_features = mi_series.nlargest(15).index  # Select top 15 features
X = data[top_features]
y = data['diagnosis']

# Step 6: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

# Step 9: Hyperparameter tuning for Random Forest using RandomizedSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
random_search_rf = RandomizedSearchCV(rf, param_grid_rf, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = random_search_rf.best_estimator_

# Step 10: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
random_search_xgb = RandomizedSearchCV(xgb, param_grid_xgb, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = random_search_xgb.best_estimator_

# Step 11: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 12: Implement Stacking Classifier with Neural Network as Meta Learner
meta_clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 13: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 14: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

for model, scores in metrics.items():
    print(f"\n{model} Performance:")
    for metric, value in scores.items():
        print(f"{metric}: {value:.4f}")
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_rf if model == "Random Forest" else y_pred_xgb if model == "XGBoost" else y_pred_ensemble if model == "Ensemble" else y_pred_stacking))

# Step 15: Feature Importance Visualization
feature_importance = best_rf.feature_importances_
sns.barplot(x=feature_importance, y=top_features)
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance for Random Forest")
plt.show()

# Step 16: SHAP for Explainability
explainer = shap.Explainer(best_xgb)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=top_features)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neural_network import MLPClassifier
import shap

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Feature Selection using Mutual Information
mi_scores = mutual_info_classif(data.drop(columns=['diagnosis']), data['diagnosis'])
mi_series = pd.Series(mi_scores, index=data.drop(columns=['diagnosis']).columns)
top_features = mi_series.nlargest(15).index  # Select top 15 features
X = data[top_features]
y = data['diagnosis']

# Step 6: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

# Step 9: Hyperparameter tuning for Random Forest using RandomizedSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
random_search_rf = RandomizedSearchCV(rf, param_grid_rf, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = random_search_rf.best_estimator_

# Step 10: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
random_search_xgb = RandomizedSearchCV(xgb, param_grid_xgb, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = random_search_xgb.best_estimator_

# Step 11: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 12: Implement Stacking Classifier with Neural Network as Meta Learner
meta_clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 13: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 14: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

# Step 15: Generate Bar Graph to Compare Model Performance
# metrics_df = pd.DataFrame(metrics).T
# metrics_df.plot(kind='bar', figsize=(10, 6))
# plt.title("Comparison of Model Performance")
# plt.xlabel("Models")
# plt.ylabel("Score")
# plt.xticks(rotation=45)
# plt.legend(loc='lower right')
# plt.show()


# Step 16: Feature Importance Visualization
feature_importance = best_rf.feature_importances_
sns.barplot(x=feature_importance, y=top_features)
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance for Random Forest")
plt.show()

# Step 17: SHAP for Explainability
explainer = shap.Explainer(best_xgb)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=top_features)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neural_network import MLPClassifier
import shap

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Feature Selection using Mutual Information
mi_scores = mutual_info_classif(data.drop(columns=['diagnosis']), data['diagnosis'])
mi_series = pd.Series(mi_scores, index=data.drop(columns=['diagnosis']).columns)
top_features = mi_series.nlargest(15).index  # Select top 15 features
X = data[top_features]
y = data['diagnosis']

# Step 6: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

# Step 9: Hyperparameter tuning for Random Forest using RandomizedSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
random_search_rf = RandomizedSearchCV(rf, param_grid_rf, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = random_search_rf.best_estimator_

# Step 10: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
random_search_xgb = RandomizedSearchCV(xgb, param_grid_xgb, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = random_search_xgb.best_estimator_

# Step 11: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 12: Implement Stacking Classifier with Neural Network as Meta Learner
meta_clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 13: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 14: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

# Step 15: Generate Bar Graph to Compare Model Performance
metrics_df = pd.DataFrame(metrics).T
plt.figure(figsize=(10, 6))
metrics_df.plot(kind='bar', figsize=(10, 6), alpha=0.85)
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Horizontal dashed lines for better comparison
plt.title("Comparison of Model Performance")
plt.xlabel("Models")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.show()

# Step 16: Feature Importance Visualization
feature_importance = best_rf.feature_importances_
sns.barplot(x=feature_importance, y=top_features)
plt.grid(axis='x', linestyle='--', alpha=0.7)  # Adding grid to feature importance plot
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance for Random Forest")
plt.show()

# Step 17: SHAP for Explainability
explainer = shap.Explainer(best_xgb)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=top_features)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neural_network import MLPClassifier
import shap

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Feature Selection using Mutual Information
mi_scores = mutual_info_classif(data.drop(columns=['diagnosis']), data['diagnosis'])
mi_series = pd.Series(mi_scores, index=data.drop(columns=['diagnosis']).columns)
top_features = mi_series.nlargest(15).index  # Select top 15 features
X = data[top_features]
y = data['diagnosis']

# Step 6: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

# Step 9: Hyperparameter tuning for Random Forest using RandomizedSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
random_search_rf = RandomizedSearchCV(rf, param_grid_rf, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = random_search_rf.best_estimator_

# Step 10: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
random_search_xgb = RandomizedSearchCV(xgb, param_grid_xgb, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = random_search_xgb.best_estimator_

# Step 11: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 12: Implement Stacking Classifier with Neural Network as Meta Learner
meta_clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 13: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)

# Step 14: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking"], [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

# Step 15: Generate Bar Graph to Compare Model Performance
metrics_df = pd.DataFrame(metrics).T
plt.figure(figsize=(10, 6))
metrics_df.plot(kind='bar', figsize=(10, 6), alpha=0.85, edgecolor='black')
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Horizontal dashed lines for better comparison
plt.title("Comparison of Model Performance")
plt.xlabel("Models")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
import shap

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Feature Selection using Mutual Information
mi_scores = mutual_info_classif(data.drop(columns=['diagnosis']), data['diagnosis'])
mi_series = pd.Series(mi_scores, index=data.drop(columns=['diagnosis']).columns)
top_features = mi_series.nlargest(15).index  # Select top 15 features
X = data[top_features]
y = data['diagnosis']

# Step 6: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

# Step 9: Train Traditional Models
knn = KNeighborsClassifier()
nb = GaussianNB()
lr = LogisticRegression()

knn.fit(X_train, y_train)
nb.fit(X_train, y_train)
lr.fit(X_train, y_train)

# Step 10: Train Optimized Random Forest using RandomizedSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
random_search_rf = RandomizedSearchCV(rf, param_grid_rf, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = random_search_rf.best_estimator_

# Step 11: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
random_search_xgb = RandomizedSearchCV(xgb, param_grid_xgb, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = random_search_xgb.best_estimator_

# Step 12: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 13: Implement Stacking Classifier with Neural Network as Meta Learner
meta_clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 14: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)
y_pred_knn = knn.predict(X_test)
y_pred_nb = nb.predict(X_test)
y_pred_lr = lr.predict(X_test)

# Step 15: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "Stacking", "KNN", "Naïve Bayes", "Logistic Regression"],
                              [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_stacking, y_pred_knn, y_pred_nb, y_pred_lr]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

# Step 16: Generate Bar Graph to Compare Model Performance
metrics_df = pd.DataFrame(metrics).T
metrics_df.plot(kind='bar', figsize=(12, 6))
plt.title("Comparison of Model Performance")
plt.xlabel("Models")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
import shap

# Step 1: Load the dataset
data = pd.read_csv("breastcancer.csv")  # Update with your actual file path

# Step 2: Explore the dataset
print("Dataset Overview:\n", data.head())
print("\nDataset Info:\n")
data.info()
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Drop irrelevant columns (e.g., ID column if present)
if 'id' in data.columns:
    data.drop(columns=['id'], inplace=True)

# Step 4: Convert categorical target variable to numerical (Malignant = 1, Benign = 0)
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})

# Step 5: Feature Selection using Mutual Information
mi_scores = mutual_info_classif(data.drop(columns=['diagnosis']), data['diagnosis'])
mi_series = pd.Series(mi_scores, index=data.drop(columns=['diagnosis']).columns)
top_features = mi_series.nlargest(15).index  # Select top 15 features
X = data[top_features]
y = data['diagnosis']

# Step 6: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 7: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Step 8: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

# Step 9: Train Traditional Models
knn = KNeighborsClassifier()
nb = GaussianNB()
lr = LogisticRegression()

knn.fit(X_train, y_train)
nb.fit(X_train, y_train)
lr.fit(X_train, y_train)

# Step 10: Train Optimized Random Forest using RandomizedSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
random_search_rf = RandomizedSearchCV(rf, param_grid_rf, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_rf.fit(X_train, y_train)

# Get the best Random Forest model
best_rf = random_search_rf.best_estimator_

# Step 11: Train and Optimize XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
random_search_xgb = RandomizedSearchCV(xgb, param_grid_xgb, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
random_search_xgb.fit(X_train, y_train)

# Get the best XGBoost model
best_xgb = random_search_xgb.best_estimator_

# Step 12: Implement Ensemble Learning (Voting Classifier)
voting_clf = VotingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

# Step 13: Implement Stacking Classifier with Neural Network as Meta Learner
meta_clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
stacking_clf = StackingClassifier(estimators=[('rf', best_rf), ('xgb', best_xgb)], final_estimator=meta_clf, passthrough=True)
stacking_clf.fit(X_train, y_train)

# Step 14: Make predictions
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)
y_pred_ensemble = voting_clf.predict(X_test)
y_pred_stacking = stacking_clf.predict(X_test)
y_pred_knn = knn.predict(X_test)
y_pred_nb = nb.predict(X_test)
y_pred_lr = lr.predict(X_test)

# Step 15: Evaluate all models
metrics = {}
for model_name, y_pred in zip(["Random Forest", "XGBoost", "Ensemble", "KNN", "Naïve Bayes", "Logistic Regression", "Stacking"],
                              [y_pred_rf, y_pred_xgb, y_pred_ensemble, y_pred_knn, y_pred_nb, y_pred_lr, y_pred_stacking]):
    metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

# Step 16: Generate Bar Graph to Compare Model Performance
metrics_df = pd.DataFrame(metrics).T
metrics_df.plot(kind='bar', figsize=(12, 6))
plt.title("Comparison of Model Performance")
plt.xlabel("Models")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Feature importance values (from Random Forest or Mutual Information)
feature_names = ["Mean Radius", "Mean Perimeter", "Mean Area", "Mean Texture", "Mean Smoothness",
                 "Compactness Mean", "Concavity Mean", "Symmetry Mean", "Fractal Dimension Mean",
                 "Concave Points Mean", "Radius SE", "Perimeter SE", "Area SE", "Texture SE", "Smoothness SE"]

importance_scores = [0.28, 0.21, 0.18, 0.12, 0.09, 0.07, 0.06, 0.05, 0.04,
                     0.03, 0.025, 0.022, 0.019, 0.017, 0.015]

# Create a bar plot
plt.figure(figsize=(12, 6))
sns.barplot(x=importance_scores, y=feature_names, palette="viridis")

# Add labels and title
plt.xlabel("Feature Importance Score")
plt.ylabel("Feature Name")
plt.title("Feature Importance Scores for Breast Cancer Detection")

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Feature importance values (from Random Forest or Mutual Information)
feature_names = ["Mean Radius", "Mean Perimeter", "Mean Area", "Mean Texture", "Mean Smoothness",
                 "Compactness Mean", "Concavity Mean", "Symmetry Mean", "Fractal Dimension Mean",
                 "Concave Points Mean", "Radius SE", "Perimeter SE", "Area SE", "Texture SE", "Smoothness SE"]

importance_scores = [0.28, 0.21, 0.18, 0.12, 0.09, 0.07, 0.06, 0.05, 0.04,
                     0.03, 0.025, 0.022, 0.019, 0.017, 0.015]

# Create a bar plot
plt.figure(figsize=(5, 5))
sns.barplot(x=feature_names, y=importance_scores, palette="viridis")

# Add labels and title
plt.xlabel("Feature Name")
plt.ylabel("Feature Importance Score")
plt.title("Feature Importance Scores for Breast Cancer Detection")

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

# Show the plot
plt.show()